{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaron-abrams-uva/DS1002-S24/blob/main/Pandas/Data_deep_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vFianT6GSxu"
      },
      "source": [
        "# Pandas Data Cleaning\n",
        "\n",
        "```\n",
        "  University of Virginia\n",
        "  DS1002: Programming for Data Science\n",
        "```\n",
        "\n",
        "## PREREQUISITES\n",
        "- data types\n",
        "- pandas dataframes\n",
        "\n",
        "## OBJECTIVES\n",
        "- Deal with issues with numerical, text, and categorical data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dad7f7QGSxw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with duplicate data\n",
        "\n",
        "Pandas can identify duplicate rows and duplicate columns within a DataFrame."
      ],
      "metadata": {
        "id": "JM2Ubx2A4y_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the file `dupe-vals.csv` in the `data/` folder on GitHub."
      ],
      "metadata": {
        "id": "wUayKYOS7i9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dupes = pd.read_csv('./dupe-vals.csv')"
      ],
      "metadata": {
        "id": "xOQjNkLN4Buj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dupes.duplicated()"
      ],
      "metadata": {
        "id": "LcGuLTgO4vCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's get a total of how many rows are duplicate\n",
        "print(dupes.duplicated().sum())"
      ],
      "metadata": {
        "id": "cjenTkCL5Fwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the duplicate rows - but be sure to keep one copy!\n",
        "dupes = dupes.drop_duplicates()"
      ],
      "metadata": {
        "id": "gq3TlCzU5IBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Missing Data\n",
        "\n",
        "Pandas primarily uses the data type `np.nan` from NumPy to represent missing data.\n"
      ],
      "metadata": {
        "id": "FaOko17S5MSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_miss = pd.DataFrame({\n",
        "    'x':[2, np.nan, 1],\n",
        "    'y':[np.nan, np.nan, 6],\n",
        "    'z':[4, np.nan, np.nan]}\n",
        ")"
      ],
      "metadata": {
        "id": "OixaGFf65cBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_miss"
      ],
      "metadata": {
        "id": "YcI14l6G5fjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `.dropna()`\n",
        "\n",
        "This will drop all rows with missing data in any column.\n",
        "\n",
        "[Details](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)"
      ],
      "metadata": {
        "id": "86S4xr5q5lPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_drop_all = df_miss.dropna()\n",
        "df_drop_all"
      ],
      "metadata": {
        "id": "INj5TKq05mId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `subset` parameter takes a list of column names to specify which columns should have missing values."
      ],
      "metadata": {
        "id": "hv0y2paq5pHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_drop_x = df_miss.dropna(subset=['x'])\n",
        "df_drop_x"
      ],
      "metadata": {
        "id": "WmsUzFeH5rHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `.fillna()`\n",
        "\n",
        "This will replace missing values with whatever you set it to, e.g. $0$s.\n",
        "\n",
        "[Details](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html)\n",
        "\n",
        "We can pass the results of an operation -- for example to peform simple imputation, we can replace missing values in each column with the median value of the respective column:"
      ],
      "metadata": {
        "id": "IDERhchK5xN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled = df_miss.fillna(df_miss.median())"
      ],
      "metadata": {
        "id": "qsqGg-HY5yaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled"
      ],
      "metadata": {
        "id": "y6EH71vQ51KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try It Yourself\n",
        "\n",
        "Fetch the `very-messy-data.csv` file from the `data/` folder on GitHub, and drag it into the files of this notebook.\n",
        "\n",
        "Using the methods you learned above, practice your skills by doing the following:\n",
        "\n",
        "1. Remove duplicate lines\n",
        "2. Where you are missing species data, remove those lines.\n",
        "3. Where you are missing other data, impute data based on the mean of the rest of that column. Repeat for any columns with missing data."
      ],
      "metadata": {
        "id": "tXyumqX457ZO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw-BehCwGSxw"
      },
      "source": [
        "## Data Type Constraints\n",
        "\n",
        "We need to make sure our variables have the correct data types, other wise we risk compromising our analysis.\n",
        "\n",
        "Example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV3Q1jYIGSxw"
      },
      "outputs": [],
      "source": [
        "# import `sales.csv`\n",
        "sales = pd.read_csv('https://ds1002-resources.s3.amazonaws.com/data/sales.csv')\n",
        "sales.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAsRcRISGSxw"
      },
      "source": [
        "We want to calculate total revenue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-W_viGiGSxw"
      },
      "outputs": [],
      "source": [
        "sales['Revenue'].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEtACOZ5GSxw"
      },
      "source": [
        "This produces some sort of numerical/repeating error we need to solve. Let's examine the data types of the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-ENZXWaGSxw"
      },
      "outputs": [],
      "source": [
        "sales.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWygc11PGSxx"
      },
      "source": [
        "And next let's look at some rows to see what we find."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWyDEx1BGSxx"
      },
      "outputs": [],
      "source": [
        "sales.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEgaUhc2GSxx"
      },
      "outputs": [],
      "source": [
        "# remove $ from Revenue columns\n",
        "sales['Revenue'] = sales['Revenue'].str.strip('$')\n",
        "sales['Revenue'] = sales['Revenue'].astype('int')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sales.head(5)"
      ],
      "metadata": {
        "id": "yTXDnFP7ZR2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales.dtypes"
      ],
      "metadata": {
        "id": "Ln-xLKN0ZabC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXGYZVb-GSxx"
      },
      "outputs": [],
      "source": [
        "# verify that Revenue is now an integer\n",
        "sales['Revenue'].dtype == 'int'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTsLqHR6GSxx"
      },
      "source": [
        "## Numeric or categorical data\n",
        "\n",
        "Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGH_wUZhGSxx"
      },
      "outputs": [],
      "source": [
        "# import `marriage_status.csv`\n",
        "marriage_status = pd.read_csv('https://ds1002-resources.s3.amazonaws.com/data/marriage_status.csv')\n",
        "marriage_status.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CbAg7bHGSxx"
      },
      "source": [
        "`0` = Never Married\n",
        "`1` = Married\n",
        "`2` = Separated\n",
        "`3` = Divorced"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "marriage_status.head(50)"
      ],
      "metadata": {
        "id": "QAC0Ut6va-pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQGA4mYVGSxx"
      },
      "outputs": [],
      "source": [
        "marriage_status['marriage_status'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UWRdY1HGSxx"
      },
      "source": [
        "<!-- Calculated as a numeric variables when in reality a categorical.  \n",
        "\n",
        "Let's change the the data type to `categorical`\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBBzNfncGSxx"
      },
      "outputs": [],
      "source": [
        "marriage_status['marriage_status'] = marriage_status['marriage_status'].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXxzT8zRGSxx"
      },
      "outputs": [],
      "source": [
        "marriage_status.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "marriage_status.dtypes"
      ],
      "metadata": {
        "id": "xRs6bC0jfmH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6majwcKGSxx"
      },
      "source": [
        "Pandas now gives summary outputs consistent with a `categorical` variable.\n",
        "\n",
        "Now let's udpate the values in that column for consistency."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "marriage_status[\"marriage_status\"] = marriage_status[\"marriage_status\"].str.lower()"
      ],
      "metadata": {
        "id": "sTVB9L1Wfjdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "marriage_status.describe()"
      ],
      "metadata": {
        "id": "fHZWY0tqf1td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This got us from 6 to the 4 unique values we want (unmarried, married, separated, divorced)"
      ],
      "metadata": {
        "id": "AyvF8EPGf6WG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY2ewRnyGSxx"
      },
      "source": [
        "## Out of Range Data\n",
        "\n",
        "Out of range data can occur from human error, data collection error, etc.   \n",
        "\n",
        "Let's work through a couple examples\n",
        "\n",
        "**`Movie Ratings`**  \n",
        "\n",
        "consists of `name`, `year`, and `score` (0-10)  \n",
        "\n",
        "**`User Signups`**  \n",
        "\n",
        "consists of `subscription_date`, `user_name`, `country`  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x47ZrpRzGSxx"
      },
      "outputs": [],
      "source": [
        "# import movie_ratings.csv\n",
        "movies = pd.read_csv('https://ds1002-resources.s3.amazonaws.com/data/movies.csv')\n",
        "movies.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iwj2JjDLGSxx"
      },
      "outputs": [],
      "source": [
        "# data viz\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(movies['Score'])\n",
        "plt.title('Average rating of top 50 movies (0-10')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmbpSE2AGSxx"
      },
      "outputs": [],
      "source": [
        "# import `user_signups`\n",
        "users = pd.read_csv('https://ds1002-resources.s3.amazonaws.com/data/user_signups.csv')\n",
        "users.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDhl6myXGSxy"
      },
      "outputs": [],
      "source": [
        "users.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tEB-aN_GSxy"
      },
      "outputs": [],
      "source": [
        "#import datetime\n",
        "import datetime as dt\n",
        "\n",
        "#convert object to date\n",
        "users['subscription_date'] = pd.to_datetime(users['subscription_date'])\n",
        "users.dtypes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzhAwiLDGSxy"
      },
      "outputs": [],
      "source": [
        "users.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9msJZIE0GSxy"
      },
      "outputs": [],
      "source": [
        "#convert object to date\n",
        "users['subscription_date'] = pd.to_datetime(users['subscription_date']).dt.date\n",
        "users.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6iU78VBGSxy"
      },
      "outputs": [],
      "source": [
        "today_date = dt.date.today()\n",
        "users[users['subscription_date'] > today_date]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8XO7MR9GSxy"
      },
      "source": [
        "## How to deal without out of range data\n",
        "\n",
        "1. Drop data\n",
        " - not advised unless very small propotion of data\n",
        " - may be removing otherwise important data\n",
        "\n",
        "2. Treat data as missing\n",
        "- can then impute data\n",
        "\n",
        "3. Set a custom value depending on the business assumptions\n",
        "\n",
        "**always document decision and steps!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6oZFbA-GSxy"
      },
      "outputs": [],
      "source": [
        "# Let's look at the `movies` df\n",
        "movies[movies['Score'] > 10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD6oCOlPGSxy"
      },
      "source": [
        "## Dropping the values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dr3pb-h3GSxy"
      },
      "outputs": [],
      "source": [
        "# by filtering\n",
        "movies1 = movies[movies.Score <= 10]\n",
        "\n",
        "#check that values were dropped\n",
        "movies1.sort_values(['Score'], ascending = False).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoEWP4i0GSxy"
      },
      "outputs": [],
      "source": [
        "# with .drop() function\n",
        "movies2 = movies.drop(movies[movies['Score'] > 10].index)\n",
        "movies2.sort_values('Score', ascending = False).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcZlz1fUGSxy"
      },
      "outputs": [],
      "source": [
        "# convert Score > 10 to 10\n",
        "movies.loc[movies['Score']> 10, 'Score'] = 10\n",
        "\n",
        "assert movies['Score'].max() <= 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8KJ8l4YGSxy"
      },
      "source": [
        "## Categorical Data & Membership Constraints\n",
        "\n",
        "- Has a predefined set of categories\n",
        "\n",
        "- Value can only be one of the membership categories\n",
        "\n",
        "- Often coded as numbers for further analysis techniques (like machine learning)\n",
        "\n",
        "### Concerns in categorical data\n",
        "\n",
        "1. Errors occur when observations have values that go beyond the predefined catogories\n",
        "\n",
        "2. Errors also occur with inconsistent fields\n",
        "\n",
        "3. Needing to collapse categories\n",
        "\n",
        "4. Data type not being defined as 'category'\n",
        "\n",
        "### Fixing observations that go beyond predefined categories\n",
        "\n",
        "- We can drop, remap, or infer categories to fix\n",
        "\n",
        "- Here is more complex example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGbe1gtiGSxy"
      },
      "outputs": [],
      "source": [
        "# import csv's\n",
        "\n",
        "study = pd.read_csv('https://ds1002-resources.s3.amazonaws.com/data/study.csv')\n",
        "categories = pd.read_csv('https://ds1002-resources.s3.amazonaws.com/data/blood_categories.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKcTpTRkGSx2"
      },
      "outputs": [],
      "source": [
        "study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epS0BkmBGSx2"
      },
      "outputs": [],
      "source": [
        "categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3Y-R7aPGSx2"
      },
      "source": [
        "We can check for errors with using joins.\n",
        "\n",
        "![](https://ds1002-resources.s3.amazonaws.com/images/joins.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhF0DlwyGSx2"
      },
      "source": [
        "**So a left anti join between study and categories would give us this:**\n",
        "\n",
        "![](https://ds1002-resources.s3.amazonaws.com/images/antijoin.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlksXM7nGSx2"
      },
      "source": [
        "**An inner join between study and category would give us:**\n",
        "\n",
        "![](https://ds1002-resources.s3.amazonaws.com/images/innerjoin.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkvETiBkGSx2"
      },
      "source": [
        "**Let's do this in python**\n",
        "\n",
        "**`.set()`**  \n",
        "**`.difference()`**\n",
        "\n",
        "[Geeks for Geeks](https://www.geeksforgeeks.org/python-set-difference/)\n",
        "\n",
        "*Note: these are from python not pandas*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(study['blood_type'])"
      ],
      "metadata": {
        "id": "X7ffyStpEEHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ht9808P8GSx2"
      },
      "outputs": [],
      "source": [
        "# find inconsistent categories\n",
        "\n",
        "inconsistent_categories = set(study['blood_type']).difference(categories['blood_type'])\n",
        "inconsistent_categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjWS0PhTGSx2"
      },
      "outputs": [],
      "source": [
        "# find inconsistent rows\n",
        "\n",
        "inconsistent_rows = study['blood_type'].isin(inconsistent_categories) # gives a boolean series\n",
        "study[inconsistent_rows] # subset study dataframe based on boolean values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0qHJb0MGSx2"
      },
      "outputs": [],
      "source": [
        "# one way to drop inconsistent rows (recall there are other ways too!)\n",
        "consistent_data = study[~inconsistent_rows]\n",
        "consistent_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzmlm7_XGSx2"
      },
      "source": [
        "### Fixing value inconsistency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1RoLQWSGSx2"
      },
      "outputs": [],
      "source": [
        "# import `marriage_status` dataset\n",
        "marriage = pd.read_csv('https://ds1002-resources.s3.amazonaws.com/data/marriage_status.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjrFWAudGSx2"
      },
      "outputs": [],
      "source": [
        "marriage.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m35gV88SGSx2"
      },
      "outputs": [],
      "source": [
        "marriage['marriage_status'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE5PyX9DGSx2"
      },
      "source": [
        "For this example we can either capitalize or lowercase the values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZJjNXDdGSx2"
      },
      "outputs": [],
      "source": [
        "# Capitalize\n",
        "marriage['marriage_status'] = marriage['marriage_status'].str.upper()\n",
        "marriage['marriage_status'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFcGjN88GSx3"
      },
      "outputs": [],
      "source": [
        "# Lowercase\n",
        "marriage['marriage_status'] = marriage['marriage_status'].str.lower()\n",
        "marriage['marriage_status'].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIHithQaGSx3"
      },
      "source": [
        "### Collapsing data into categories\n",
        "\n",
        "* Often we will need to distill continuous data into categories\n",
        "* Categories should have evidence-based backing behing them\n",
        "* Cateogires can increase interpretability but can also lose valuable information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDcJql-CGSx3"
      },
      "outputs": [],
      "source": [
        "income = pd.read_csv('https://ds1002-resources.s3.amazonaws.com/data/income.csv')\n",
        "income"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdS0yCeXGSx3"
      },
      "source": [
        "**`.qcut()`**\n",
        "\n",
        "* automatically divides data into categories based on the argument `q` and the distribution of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mbfrra54GSx3"
      },
      "outputs": [],
      "source": [
        "# group_names = ['0-100K', '100K-250K', '250K-500K', '500K+']\n",
        "income['income_group'] = pd.qcut(income['household_income'], q = 4) #,\n",
        "#                                     labels = group_names)\n",
        "\n",
        "income"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHWQRZaIGSx3"
      },
      "source": [
        "**`.cut()`**\n",
        "\n",
        "* Allows you to use categories' cut-off ranges with the `bins` argument"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBn-3Gl7GSx3"
      },
      "outputs": [],
      "source": [
        "ranges = [0, 100000, 500000, np.inf]\n",
        "group_names = ['0-100K', '100K-500K', '500K+']\n",
        "income['income_group'] = pd.cut(income['household_income'], bins = ranges,\n",
        "                                     labels = group_names)\n",
        "\n",
        "income"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDLssO_NGSx3"
      },
      "source": [
        "## Map categories into fewer ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIr2JbqDGSx3"
      },
      "outputs": [],
      "source": [
        "computer = pd.read_csv('https://ds1002-resources.s3.amazonaws.com/data/computer.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "computer"
      ],
      "metadata": {
        "id": "x-6pFenDHDf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHsclMPiGSx3"
      },
      "source": [
        "We want to collapse the `operating system` column into `'DesktopOS', 'MobileOS'`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWNFplsnGSx3"
      },
      "outputs": [],
      "source": [
        "# create dictionary\n",
        "mapping = {'Microsoft': 'DesktopOS', 'MacOS': 'DesktopOS', 'Linux': 'DesktopOS', 'IOS': 'MobileOS', 'Android': 'MobileOS'}\n",
        "\n",
        "# use `.replace`\n",
        "computer['operating_system_category'] = computer['operating_system'].replace(mapping)\n",
        "computer['operating_system_category'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcIiJ_I5GSx3"
      },
      "outputs": [],
      "source": [
        "computer.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB3AdOU7GSx3"
      },
      "source": [
        "## Text Data\n",
        "\n",
        "* Common type of data\n",
        "\n",
        "* Common text data problems are:\n",
        "> 1. data inconsistency\n",
        "> 2. fixed length violations\n",
        "> 3. typos\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_7qSH3BGSx3"
      },
      "outputs": [],
      "source": [
        "phones = pd.read_csv('https://ds1002-resources.s3.amazonaws.com/data/phone_numbers.csv')\n",
        "phones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zly4b0lPGSx3"
      },
      "source": [
        "Ideally we want to remove dashes, have each phone number start with the full country code, and remove phone numbers that don't have full values listed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrMNKSI4GSx3"
      },
      "outputs": [],
      "source": [
        "# Replace the '+' with '00'\n",
        "phones['phone_number'] = phones['phone_number'].str.replace('+', '00')\n",
        "phones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQlEz7soGSx3"
      },
      "outputs": [],
      "source": [
        "# Replace the '-' with nothing\n",
        "phones['phone_number'] = phones['phone_number'].str.replace('-', '')\n",
        "phones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVgYcKpZGSx3"
      },
      "outputs": [],
      "source": [
        "# Replace phone numbers with lower than 10 digits to NaN\n",
        "digits = phones['phone_number'].str.len() # gets the length of the each phone number\n",
        "phones.loc[digits < 10, 'phone_number'] = np.nan # subset phone numbers with less than 10 digits, replace with NaN\n",
        "phones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2TuKjK1GSx3"
      },
      "outputs": [],
      "source": [
        "# checking data with assert statements\n",
        "\n",
        "# find length of each row in phone_number columns\n",
        "sanity_check = phones['phone_number'].str.len()\n",
        "\n",
        "# assert minimum phone_number lenth is 10\n",
        "assert sanity_check.min() >= 10\n",
        "\n",
        "# assert all number do not have a '+' or '-'\n",
        "assert phones['phone_number'].str.contains('-').any() == False\n",
        "assert phones['phone_number'].str.contains('+').any() == False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZSZFkViGSx3"
      },
      "outputs": [],
      "source": [
        "phones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGOaXkzxGSx3"
      },
      "source": [
        "### More complicated regular expression (`regex`) examples\n",
        "\n",
        "* Regular expressions give us the ability to search for any pattern in text data, like only digits for example\n",
        "\n",
        "* They are like control + find in your browser, but *much* more dynamic and robust\n",
        "\n",
        "* Read more about them starting with [wikipedia](https://en.wikipedia.org/wiki/Regular_expression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8cRDS0aGSx3"
      },
      "outputs": [],
      "source": [
        "phones_complex = pd.read_csv('https://ds1002-resources.s3.amazonaws.com/data/phone_numbers_complex.csv')\n",
        "phones_complex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBZFVQftGSx4"
      },
      "outputs": [],
      "source": [
        "# replace letters with nothing\n",
        "\n",
        "phones_complex['phone_number'] = phones_complex['phone_number'].str.replace('[^0-9]', '')\n",
        "#phones_complex['phone_number'] = phones_complex['phone_number'].str.replace(r'\\D+', '') #\\D+ mean anything that is not a digit, found in regex library\n",
        "phones_complex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0JM4xXqGSx4"
      },
      "source": [
        "## Data Uniformity\n",
        "\n",
        "* We want data within columns to have the same units (temperature, weight, money)\n",
        "\n",
        "* Or data, as dates, to have the same format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3TO_pQ0GSx4"
      },
      "outputs": [],
      "source": [
        "temps = pd.read_csv('https://ds1002-resources.s3.amazonaws.com/data/temperatures.csv')\n",
        "temps.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_lLJrbdGSx4"
      },
      "source": [
        "**Let's look at graph**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nkrse3DGSx4"
      },
      "outputs": [],
      "source": [
        "# import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a scatter plot\n",
        "plt.scatter(x = 'date', y = 'temperature', data = temps)\n",
        "\n",
        "# create title, xlabel, and ylabel\n",
        "plt.title('Temperatures in Celsius March 2019 - NYC')\n",
        "plt.xlabel('date')\n",
        "plt.ylabel('temperature (degrees Celsius)')\n",
        "plt.xticks(rotation = 90)\n",
        "\n",
        "# show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_s9IFLGGSx4"
      },
      "source": [
        "**Convert farhenheit data to celsius**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMSODRy6GSx4"
      },
      "outputs": [],
      "source": [
        "temp_fah = temps.loc[temps['temperature'] > 40, 'temperature'] # use .loc to subset data above 40\n",
        "\n",
        "temp_cels = (temp_fah - 32) * (5/9) # convert to celsius\n",
        "\n",
        "temps.loc[temps['temperature'] > 40, 'temperature'] = temp_cels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lamYfrejGSx4"
      },
      "outputs": [],
      "source": [
        "# create a scatter plot\n",
        "plt.scatter(x = 'date', y = 'temperature', data = temps)\n",
        "\n",
        "# create title, xlabel, and ylabel\n",
        "plt.title('Temperatures in Celsius March 2019 - NYC')\n",
        "plt.xlabel('date')\n",
        "plt.ylabel('temperature (degrees Celsius)')\n",
        "plt.xticks(rotation = 90)\n",
        "\n",
        "# show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqc7o-GUGSx4"
      },
      "source": [
        "**Clean-up Dates**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j35iZ2XYGSx4"
      },
      "outputs": [],
      "source": [
        "birthdays = pd.read_csv('https://ds1002-resources.s3.amazonaws.com/data/birthdays.csv')\n",
        "birthdays.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jN1ZdaWGSx4"
      },
      "source": [
        "The `datetime` package is useful for representing dates\n",
        "\n",
        "[How to format dates in python](https://stackabuse.com/how-to-format-dates-in-python/)\n",
        "\n",
        "We also use `pandas.to_datetime`\n",
        "* can recognize more formats automatically\n",
        "* sometimes fails with erroneous or unrecongizable formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3oAFltLGSx4"
      },
      "outputs": [],
      "source": [
        "# converts to datetime\n",
        "birthdays['birth_date'] = pd.to_datetime(birthdays['birth_date'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM2yH8XhGSx4"
      },
      "source": [
        "**Doesn't work!**\n",
        "\n",
        "try:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0k8jdPBGSx4"
      },
      "outputs": [],
      "source": [
        "birthdays['birth_date'] = pd.to_datetime(birthdays['birth_date'],\n",
        "                                        # attempt to infer format for each date\n",
        "                                        infer_datetime_format = True,\n",
        "                                        # return NA for rows where conversion failed\n",
        "                                        errors = 'coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLI7iFceGSx4"
      },
      "outputs": [],
      "source": [
        "birthdays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZBeY-e2GSx4"
      },
      "outputs": [],
      "source": [
        "birthdays['birth_date'] = birthdays['birth_date'].dt.strftime('%d-%m-%Y')\n",
        "birthdays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_83y5yHNGSx4"
      },
      "source": [
        "**Ambiguous Data**\n",
        "\n",
        "Is `2019-03-08` in August or March?\n",
        "\n",
        "* Can convert to `NaN` or treat accordingly\n",
        "* Can infer - this is where knowing your data is useful"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}